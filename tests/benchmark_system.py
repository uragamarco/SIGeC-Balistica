#!/usr/bin/env python3
"""
Sistema de Benchmark Completo - SEACABAr
========================================

Script para medir el rendimiento del sistema despuÃ©s de las optimizaciones
implementadas. Incluye benchmarks de memoria, CPU, GPU y operaciones de matching.

Autor: SEACABAr Team
Fecha: 2024
"""

import os
import sys
import time
import psutil
import gc
import numpy as np
import cv2
from pathlib import Path
from typing import Dict, List, Tuple, Optional
import json
from dataclasses import dataclass, asdict
import logging

# AÃ±adir el directorio raÃ­z al path
sys.path.insert(0, str(Path(__file__).parent.parent))

# Importaciones del sistema
try:
    from matching.unified_matcher import UnifiedMatcher
    from matching.cmc_algorithm import CMCAlgorithm, CMCParameters
    from database.unified_database import UnifiedDatabase
    from utils.memory_cache import MemoryCache
    from utils.logger import get_logger
    IMPORTS_AVAILABLE = True
except ImportError as e:
    print(f"Error importando mÃ³dulos: {e}")
    IMPORTS_AVAILABLE = False

@dataclass
class BenchmarkResult:
    """Resultado de un benchmark individual"""
    name: str
    duration: float
    memory_peak: float
    memory_avg: float
    cpu_usage: float
    success: bool
    error_message: str = ""
    additional_metrics: Dict = None

@dataclass
class SystemBenchmarkReport:
    """Reporte completo del benchmark del sistema"""
    timestamp: str
    system_info: Dict
    benchmark_results: List[BenchmarkResult]
    overall_performance_score: float
    optimization_improvements: Dict

class SystemBenchmark:
    """Clase principal para ejecutar benchmarks del sistema"""
    
    def __init__(self):
        self.logger = get_logger("benchmark")
        self.process = psutil.Process()
        self.results = []
        
        # Configurar imÃ¡genes de prueba
        self.test_images_dir = Path(__file__).parent / "data"
        self.test_images_dir.mkdir(exist_ok=True)
        
    def create_test_images(self) -> List[Path]:
        """Crea imÃ¡genes de prueba para los benchmarks"""
        test_images = []
        
        # Crear diferentes tamaÃ±os de imÃ¡genes de prueba
        sizes = [(512, 512), (1024, 1024), (2048, 2048)]
        
        for i, (width, height) in enumerate(sizes):
            # Generar imagen sintÃ©tica con patrones balÃ­sticos simulados
            image = np.random.randint(0, 256, (height, width, 3), dtype=np.uint8)
            
            # AÃ±adir algunos patrones circulares para simular caracterÃ­sticas balÃ­sticas
            center_x, center_y = width // 2, height // 2
            for j in range(5):
                radius = 20 + j * 15
                cv2.circle(image, (center_x + j*50, center_y + j*30), radius, (255, 255, 255), 2)
            
            # AÃ±adir ruido y texturas
            noise = np.random.normal(0, 25, image.shape).astype(np.uint8)
            image = cv2.add(image, noise)
            
            # Guardar imagen
            image_path = self.test_images_dir / f"test_image_{i}_{width}x{height}.png"
            cv2.imwrite(str(image_path), image)
            test_images.append(image_path)
            
        return test_images
    
    def get_system_info(self) -> Dict:
        """Obtiene informaciÃ³n del sistema"""
        return {
            "cpu_count": psutil.cpu_count(),
            "cpu_freq": psutil.cpu_freq()._asdict() if psutil.cpu_freq() else None,
            "memory_total": psutil.virtual_memory().total,
            "memory_available": psutil.virtual_memory().available,
            "platform": sys.platform,
            "python_version": sys.version,
            "opencv_version": cv2.__version__,
            "numpy_version": np.__version__
        }
    
    def monitor_resources(self, func, *args, **kwargs):
        """Monitorea recursos durante la ejecuciÃ³n de una funciÃ³n"""
        # Limpiar memoria antes del benchmark
        gc.collect()
        
        # MÃ©tricas iniciales
        initial_memory = self.process.memory_info().rss / 1024 / 1024  # MB
        start_time = time.time()
        
        # Monitoreo durante ejecuciÃ³n
        memory_samples = []
        cpu_samples = []
        
        try:
            # Ejecutar funciÃ³n
            result = func(*args, **kwargs)
            
            # MÃ©tricas finales
            end_time = time.time()
            final_memory = self.process.memory_info().rss / 1024 / 1024  # MB
            
            # Obtener muestras de CPU
            cpu_percent = self.process.cpu_percent()
            
            return {
                'result': result,
                'duration': end_time - start_time,
                'memory_peak': final_memory,
                'memory_avg': (initial_memory + final_memory) / 2,
                'cpu_usage': cpu_percent,
                'success': True
            }
            
        except Exception as e:
            return {
                'result': None,
                'duration': time.time() - start_time,
                'memory_peak': self.process.memory_info().rss / 1024 / 1024,
                'memory_avg': initial_memory,
                'cpu_usage': 0,
                'success': False,
                'error': str(e)
            }
    
    def benchmark_unified_matcher(self, test_images: List[Path]) -> BenchmarkResult:
        """Benchmark del UnifiedMatcher"""
        self.logger.info("Ejecutando benchmark de UnifiedMatcher...")
        
        def run_matcher_benchmark():
            if not IMPORTS_AVAILABLE:
                raise ImportError("MÃ³dulos no disponibles")
                
            matcher = UnifiedMatcher()
            results = []
            
            # Probar con diferentes pares de imÃ¡genes
            for i in range(min(3, len(test_images))):
                for j in range(i+1, min(i+3, len(test_images))):
                    img1 = cv2.imread(str(test_images[i]))
                    img2 = cv2.imread(str(test_images[j]))
                    
                    if img1 is not None and img2 is not None:
                        result = matcher.compare_images(img1, img2)
                        results.append(result)
            
            return results
        
        metrics = self.monitor_resources(run_matcher_benchmark)
        
        return BenchmarkResult(
            name="UnifiedMatcher",
            duration=metrics['duration'],
            memory_peak=metrics['memory_peak'],
            memory_avg=metrics['memory_avg'],
            cpu_usage=metrics['cpu_usage'],
            success=metrics['success'],
            error_message=metrics.get('error', ''),
            additional_metrics={'comparisons': len(metrics.get('result', []))}
        )
    
    def benchmark_cmc_algorithm(self, test_images: List[Path]) -> BenchmarkResult:
        """Benchmark del algoritmo CMC"""
        self.logger.info("Ejecutando benchmark de CMC Algorithm...")
        
        def run_cmc_benchmark():
            if not IMPORTS_AVAILABLE:
                raise ImportError("MÃ³dulos no disponibles")
                
            cmc = CMCAlgorithm(CMCParameters())
            results = []
            
            # Probar con pares de imÃ¡genes
            for i in range(min(2, len(test_images))):
                for j in range(i+1, min(i+2, len(test_images))):
                    img1 = cv2.imread(str(test_images[i]), cv2.IMREAD_GRAYSCALE)
                    img2 = cv2.imread(str(test_images[j]), cv2.IMREAD_GRAYSCALE)
                    
                    if img1 is not None and img2 is not None:
                        result = cmc.compare_images(img1, img2)
                        results.append(result)
            
            return results
        
        metrics = self.monitor_resources(run_cmc_benchmark)
        
        return BenchmarkResult(
            name="CMC_Algorithm",
            duration=metrics['duration'],
            memory_peak=metrics['memory_peak'],
            memory_avg=metrics['memory_avg'],
            cpu_usage=metrics['cpu_usage'],
            success=metrics['success'],
            error_message=metrics.get('error', ''),
            additional_metrics={'cmc_comparisons': len(metrics.get('result', []))}
        )
    
    def benchmark_database_operations(self) -> BenchmarkResult:
        """Benchmark de operaciones de base de datos"""
        self.logger.info("Ejecutando benchmark de Database Operations...")
        
        def run_database_benchmark():
            if not IMPORTS_AVAILABLE:
                raise ImportError("MÃ³dulos no disponibles")
                
            db = UnifiedDatabase()
            
            # Probar operaciones bÃ¡sicas
            stats = db.get_database_stats()
            cases = db.get_cases(limit=10)
            
            # Simular bÃºsqueda
            if cases:
                case_id = cases[0]['id']
                images = db.get_images_by_case(case_id)
            
            return {'stats': stats, 'cases_count': len(cases)}
        
        metrics = self.monitor_resources(run_database_benchmark)
        
        return BenchmarkResult(
            name="Database_Operations",
            duration=metrics['duration'],
            memory_peak=metrics['memory_peak'],
            memory_avg=metrics['memory_avg'],
            cpu_usage=metrics['cpu_usage'],
            success=metrics['success'],
            error_message=metrics.get('error', ''),
            additional_metrics=metrics.get('result', {})
        )
    
    def benchmark_memory_cache(self) -> BenchmarkResult:
        """Benchmark del sistema de cachÃ© de memoria"""
        self.logger.info("Ejecutando benchmark de Memory Cache...")
        
        def run_cache_benchmark():
            if not IMPORTS_AVAILABLE:
                raise ImportError("MÃ³dulos no disponibles")
                
            cache = MemoryCache(max_memory_mb=50)
            
            # Probar operaciones de cachÃ©
            for i in range(100):
                key = f"test_key_{i}"
                value = np.random.rand(100, 100)  # Datos de prueba
                cache.put(key, value)
            
            # Probar recuperaciÃ³n
            hits = 0
            for i in range(50):
                key = f"test_key_{i}"
                if cache.get(key) is not None:
                    hits += 1
            
            stats = cache.get_stats()
            return {'hits': hits, 'cache_stats': stats}
        
        metrics = self.monitor_resources(run_cache_benchmark)
        
        return BenchmarkResult(
            name="Memory_Cache",
            duration=metrics['duration'],
            memory_peak=metrics['memory_peak'],
            memory_avg=metrics['memory_avg'],
            cpu_usage=metrics['cpu_usage'],
            success=metrics['success'],
            error_message=metrics.get('error', ''),
            additional_metrics=metrics.get('result', {})
        )
    
    def run_all_benchmarks(self) -> SystemBenchmarkReport:
        """Ejecuta todos los benchmarks y genera un reporte"""
        self.logger.info("Iniciando benchmarks completos del sistema...")
        
        # Crear imÃ¡genes de prueba
        test_images = self.create_test_images()
        
        # Ejecutar benchmarks individuales
        benchmarks = [
            self.benchmark_unified_matcher(test_images),
            self.benchmark_cmc_algorithm(test_images),
            self.benchmark_database_operations(),
            self.benchmark_memory_cache()
        ]
        
        # Calcular puntuaciÃ³n general de rendimiento
        successful_benchmarks = [b for b in benchmarks if b.success]
        if successful_benchmarks:
            avg_duration = sum(b.duration for b in successful_benchmarks) / len(successful_benchmarks)
            avg_memory = sum(b.memory_peak for b in successful_benchmarks) / len(successful_benchmarks)
            
            # PuntuaciÃ³n basada en velocidad y eficiencia de memoria (0-100)
            performance_score = max(0, 100 - (avg_duration * 10) - (avg_memory / 100))
        else:
            performance_score = 0
        
        # Crear reporte
        report = SystemBenchmarkReport(
            timestamp=time.strftime("%Y-%m-%d %H:%M:%S"),
            system_info=self.get_system_info(),
            benchmark_results=benchmarks,
            overall_performance_score=performance_score,
            optimization_improvements={
                "memory_optimizations": "Implementadas",
                "cache_system": "Activo",
                "database_optimizations": "Implementadas",
                "matching_algorithms": "Optimizados",
                "image_processing": "Mejorado"
            }
        )
        
        return report
    
    def save_report(self, report: SystemBenchmarkReport, filename: str = "benchmark_report.json"):
        """Guarda el reporte en un archivo JSON"""
        report_path = Path(__file__).parent.parent / filename
        
        # Convertir a diccionario serializable
        report_dict = asdict(report)
        
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report_dict, f, indent=2, ensure_ascii=False)
        
        self.logger.info(f"Reporte guardado en: {report_path}")
        return report_path
    
    def print_summary(self, report: SystemBenchmarkReport):
        """Imprime un resumen del benchmark"""
        print("\n" + "="*60)
        print("REPORTE DE BENCHMARK DEL SISTEMA SEACABAr")
        print("="*60)
        print(f"Timestamp: {report.timestamp}")
        print(f"PuntuaciÃ³n General: {report.overall_performance_score:.1f}/100")
        print("\nResultados por Componente:")
        print("-" * 40)
        
        for result in report.benchmark_results:
            status = "âœ“ Ã‰XITO" if result.success else "âœ— FALLO"
            print(f"{result.name:20} | {status:8} | {result.duration:.2f}s | {result.memory_peak:.1f}MB")
            if not result.success and result.error_message:
                print(f"                     Error: {result.error_message}")
        
        print("\nOptimizaciones Implementadas:")
        print("-" * 40)
        for key, value in report.optimization_improvements.items():
            print(f"â€¢ {key.replace('_', ' ').title()}: {value}")
        
        print("\nInformaciÃ³n del Sistema:")
        print("-" * 40)
        print(f"CPU Cores: {report.system_info.get('cpu_count', 'N/A')}")
        print(f"Memoria Total: {report.system_info.get('memory_total', 0) / 1024**3:.1f} GB")
        print(f"Python: {report.system_info.get('python_version', 'N/A').split()[0]}")
        print(f"OpenCV: {report.system_info.get('opencv_version', 'N/A')}")

def main():
    """FunciÃ³n principal"""
    print("Iniciando Sistema de Benchmark SEACABAr...")
    
    # Configurar logging
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    # Crear y ejecutar benchmark
    benchmark = SystemBenchmark()
    
    try:
        # Ejecutar todos los benchmarks
        report = benchmark.run_all_benchmarks()
        
        # Guardar reporte
        report_path = benchmark.save_report(report)
        
        # Mostrar resumen
        benchmark.print_summary(report)
        
        print(f"\nâœ“ Benchmark completado exitosamente!")
        print(f"ðŸ“„ Reporte detallado guardado en: {report_path}")
        
        return 0
        
    except Exception as e:
        print(f"\nâœ— Error durante el benchmark: {e}")
        logging.exception("Error en benchmark")
        return 1

if __name__ == "__main__":
    sys.exit(main())